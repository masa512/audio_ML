{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyMOMXQrzSLT1/7u1MYe2RfE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masa512/audio_ML/blob/main/basic_audio_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio Deep Learning Basics\n",
        "\n",
        "reference : https://pytorch.org/tutorials/intermediate/speech_command_classification_with_torchaudio_tutorial.html\n",
        "\n",
        "the architecture : \n",
        "https://arxiv.org/pdf/1610.00087.pdf"
      ],
      "metadata": {
        "id": "e90cN23q1BS_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader generation"
      ],
      "metadata": {
        "id": "Y6Zd9OZw5d-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio \n",
        "import torch\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "renz2hH-1IGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change device to cuda\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "47L1T24v1uRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load classification dataset class and randomly split dataset into 80% Train and 20% Test\n",
        "SC_data = torchaudio.datasets.SPEECHCOMMANDS('.', download=True)\n",
        "\n",
        "# The partitioning\n",
        "N_total = SC_data.__len__()\n",
        "N_train = int(N_total*0.8)\n",
        "N_test = N_total-N_train\n",
        "\n",
        "train_data, test_data = torch.utils.data.random_split(SC_data,[N_train,N_test])\n"
      ],
      "metadata": {
        "id": "3fRjxQYa11CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now the train, test dataloaders\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_data,\n",
        "                                           batch_size=4,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_data,\n",
        "                                           batch_size=4,\n",
        "                                           shuffle=False)"
      ],
      "metadata": {
        "id": "ATyEOBAg3SQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will look at the data content in detail\n",
        "\n",
        "'''\n",
        "train_data\n",
        "\n",
        "1. audio (X)\n",
        "2. Sample rate\n",
        "3. Classification (Y)\n",
        "4. Speaker ID\n",
        "5. Utterance number\n",
        "'''\n",
        "\n",
        "train_data[0] \n"
      ],
      "metadata": {
        "id": "NJ224wsL6HbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Observe the Data a bit more visually"
      ],
      "metadata": {
        "id": "p_oGixwe_Zsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List file names starting with char and is a folder\n",
        "\n",
        "label_names = [name for name in os.listdir(\"./SpeechCommands/speech_commands_v0.02\") if name[0].isalpha() and os.path.isdir(os.path.join('./SpeechCommands/speech_commands_v0.02',name))]\n",
        "\n",
        "# The number labels for this dataset is shown below\n",
        "print(len(label_names))\n",
        "\n",
        "# Read number of audios per label\n",
        "label_cnt = [len(os.listdir(os.path.join(\"./SpeechCommands/speech_commands_v0.02\",label))) for label in label_names]\n",
        "\n"
      ],
      "metadata": {
        "id": "Te6LJ9vm_rRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the count using pie chart \n",
        "plt.figure(figsize =(10,10))\n",
        "plt.pie(label_cnt,labels = label_names)\n",
        "plt.title('Label Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LxNQvCBHB2M9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Model : M5\n",
        "\n",
        "The model is based on elementary conv network\n"
      ],
      "metadata": {
        "id": "fI5YigpBHK81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class conv_module(nn.Module):\n",
        "  # The chain of 1D Conv -> BN -> Relu\n",
        "\n",
        "  def __init__(self,in_channel,out_channel,kernel_size,stride):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv = nn.Conv1d(in_channel, out_channel, kernel_size, stride = 1)\n",
        "    self.bn = nn.BatchNorm1d(out_channel)\n",
        "    self.relu = nn.ReLU()\n",
        "  \n",
        "  def forward(self,x):\n",
        "    x = self.conv(x)\n",
        "    x = self.bn(x)\n",
        "    return self.relu(x)\n",
        "\n",
        "class M5_model(nn.Module):\n",
        "  \n",
        "  def __init__(self,in_channel,base_channel,n_class):\n",
        "    super().__init__()\n",
        "    self.conv1 = conv_module(in_channel,base_channel,kernel_size=80, stride = 16)\n",
        "    self.conv2 = conv_module(base_channel,base_channel,kernel_size=3)\n",
        "    self.conv3 = conv_module(base_channel,2 * base_channel,kernel_size=3)\n",
        "    self.conv4 = conv_module(2 * base_channel,2 * base_channel,kernel_size=3)\n",
        "\n",
        "    self.pool = nn.MaxPool1d(4)\n",
        "    self.fc1 = nn.Linear(2 * base_channel, n_class)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    x = self.conv1(x)\n",
        "    x = self.pool(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = self.pool(x)\n",
        "\n",
        "    x = self.conv3(x)\n",
        "    x = self.pool(x)\n",
        "\n",
        "    x = self.conv4(x)\n",
        "    x = self.pool(x)\n",
        "\n",
        "    # The FC layer at the end\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "9P9cru31I3GX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}